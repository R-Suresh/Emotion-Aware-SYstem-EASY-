{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas\n",
    "import random\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense , Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename1='C:/Users/RAHUL/Downloads/motivation - Sheet1.csv'\n",
    "#filename2='C:/Users/RAHUL/Downloads/calm.csv'\n",
    "filename3='C:/Users/RAHUL/Downloads/happy - Sheet1.csv'\n",
    "filename='C:/Users/RAHUL/Downloads/compiled.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_shuffle(filename):\n",
    "    dataframe = pandas.read_csv(filename, header=None)\n",
    "    ds = dataframe.sample(frac=1)\n",
    "    dataset = ds.values\n",
    "    X = dataset[:,0]\n",
    "    Y = dataset[:,1]\n",
    "    return X,Y\n",
    "X,Y=read_csv_shuffle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_y_labels(Y):\n",
    "    y_train=[]\n",
    "    for var in Y:\n",
    "        if(var==\"Happy\"):\n",
    "            y_train.append(1)\n",
    "        else:\n",
    "            y_train.append(0)\n",
    "    return y_train\n",
    "y_train=generate_y_labels(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_sentences(X):\n",
    "    sentences=[]\n",
    "    for row in X:\n",
    "        #print(row)\n",
    "        sentences.append(tf.compat.as_str(row).split())\n",
    "    return sentences\n",
    "sentences=generate_all_sentences(X)\n",
    "#print(sentences[6]) \n",
    "#print(y_train[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_list(sentences):\n",
    "    word_list=[]\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            word_list.append(word)\n",
    "    return word_list\n",
    "#split=sentence.split()\n",
    "#print(split)\n",
    "word_list=get_word_list(sentences)\n",
    "#print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230\n",
      "782\n"
     ]
    }
   ],
   "source": [
    "#I need to convert sentences to dictionary form\n",
    "#for that I need a dict\n",
    "def file_to_word_ids(word_list,sentences,word_to_id):\n",
    "    new_sentences=[]\n",
    "    for sentence in sentences:\n",
    "        new_sentences.append([word_to_id[word] for word in sentence if word in word_to_id])\n",
    "    #data = word_list\n",
    "    return new_sentences\n",
    "\n",
    "def get_dict(word_list,sentences):\n",
    "    counter = collections.Counter(word_list)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    vocabulary = len(word_to_id)\n",
    "    reverse_dictionary = dict(zip(word_to_id.values(), word_to_id.keys()))\n",
    "    train_data = file_to_word_ids(word_list,sentences, word_to_id)\n",
    "    return word_to_id,reverse_dictionary,vocabulary, train_data\n",
    "dictionary,reverse_dictionary,vocabulary, X_train=get_dict(word_list,sentences)\n",
    "print(len(X_train))\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_review_length = 18\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=X_train[192:224]\n",
    "y_test=y_train[192:224]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train[:192]\n",
    "#Y_train=Y_train[:160]\n",
    "y_train=y_train[:192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='C:/Users/RAHUL/projects/Many_to_one'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 18, 32)            25024     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 33,377\n",
      "Trainable params: 33,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "def build_model(hidden_size=32,use_dropout=True):\n",
    "    #embedding_vecor_length = 32\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary, hidden_size, input_length=max_review_length))\n",
    "    #model.add(LSTM(hidden_size,return_sequences=True))\n",
    "    model.add(LSTM(hidden_size))\n",
    "    if use_dropout:\n",
    "        model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "def train_model(model,reload_filename='NULL',save_best_only=True,batch_size=32,num_epochs=10):\n",
    "    if(save_best_only):\n",
    "        checkpointer = ModelCheckpoint(filepath=data_path + '/model-{epoch:02d}-{loss:.4f}.hdf5', verbose=1,monitor='val_acc', save_best_only=True, mode='auto')\n",
    "    else:\n",
    "        checkpointer = ModelCheckpoint(filepath=data_path + '/model-{epoch:02d}-{loss:.4f}.hdf5', verbose=1)\n",
    "    #period option in checkpointer -> after how many epochs to save the model\n",
    "    if(reload_filename!=\"NULL\"):\n",
    "            model = load_model(data_path + \"/\" + reload_filename)\n",
    "    model.fit(X_train, y_train, validation_split=0.3, epochs=num_epochs, batch_size=batch_size,callbacks=[checkpointer])\n",
    "    model.save(data_path + \"final_model.hdf5\")\n",
    "hidden_size=32\n",
    "rnn_model=build_model()\n",
    "#add call back to save model after every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 134 samples, validate on 58 samples\n",
      "Epoch 1/10\n",
      "134/134 [==============================] - 1s 10ms/step - loss: 0.6922 - acc: 0.5746 - val_loss: 0.6940 - val_acc: 0.4828\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.48276, saving model to C:/Users/RAHUL/projects/Many_to_one/model-01-0.6922.hdf5\n",
      "Epoch 2/10\n",
      "134/134 [==============================] - 0s 585us/step - loss: 0.6897 - acc: 0.5672 - val_loss: 0.6942 - val_acc: 0.4310\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.48276\n",
      "Epoch 3/10\n",
      "134/134 [==============================] - 0s 622us/step - loss: 0.6854 - acc: 0.5970 - val_loss: 0.6936 - val_acc: 0.4310\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.48276\n",
      "Epoch 4/10\n",
      "134/134 [==============================] - 0s 719us/step - loss: 0.6821 - acc: 0.5896 - val_loss: 0.6930 - val_acc: 0.4483\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.48276\n",
      "Epoch 5/10\n",
      "134/134 [==============================] - 0s 629us/step - loss: 0.6761 - acc: 0.6642 - val_loss: 0.6919 - val_acc: 0.4655\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.48276\n",
      "Epoch 6/10\n",
      "134/134 [==============================] - 0s 678us/step - loss: 0.6693 - acc: 0.6119 - val_loss: 0.6913 - val_acc: 0.4655\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.48276\n",
      "Epoch 7/10\n",
      "134/134 [==============================] - 0s 715us/step - loss: 0.6563 - acc: 0.6343 - val_loss: 0.6905 - val_acc: 0.4655\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.48276\n",
      "Epoch 8/10\n",
      "134/134 [==============================] - 0s 585us/step - loss: 0.6386 - acc: 0.6194 - val_loss: 0.6893 - val_acc: 0.4655\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.48276\n",
      "Epoch 9/10\n",
      "134/134 [==============================] - 0s 596us/step - loss: 0.6063 - acc: 0.6791 - val_loss: 0.6861 - val_acc: 0.4655\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.48276\n",
      "Epoch 10/10\n",
      "134/134 [==============================] - 0s 603us/step - loss: 0.5622 - acc: 0.7015 - val_loss: 0.6781 - val_acc: 0.4828\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.48276 to 0.48276, saving model to C:/Users/RAHUL/projects/Many_to_one/model-10-0.5622.hdf5\n"
     ]
    }
   ],
   "source": [
    "train_model(model=rnn_model,save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 8ms/step\n",
      "Accuracy: 46.88%\n"
     ]
    }
   ],
   "source": [
    "def test_model_accuracy(filename):\n",
    "    model = load_model(data_path + \"/\" + filename)\n",
    "    scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "filename=\"binary_crossentropy/single 32 lstm/model-10-0.5622.hdf5\"\n",
    "test_model_accuracy(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
